import argparse
import gradio as gr
from gradio import utils
import random
import time
import os
import json
import function
import repo_download
from datetime import datetime

parser = argparse.ArgumentParser(description="Cloud chatbot å¯åŠ¨è„šæœ¬", add_help=True)
parser.add_argument('--port', nargs='?', const=True, help='æŒ‡å®šæœåŠ¡ç«¯å£å·')
parser.add_argument('--listen', nargs='?', const=True, help='ä»¥ 0.0.0.0 ä½œä¸ºæœåŠ¡å™¨åç§°å¯åŠ¨ Gradioï¼Œå…è®¸å“åº”ç½‘ç»œè¯·æ±‚')
parser.add_argument('--ui-test', action='store_true', help="å…è®¸åœ¨æ— æ˜¾å¡æ¨¡å¼ä¸‹å¯åŠ¨ï¼Œä»¥æµ‹è¯•UIçš„åŠŸèƒ½")
parser.add_argument('--model-set', nargs='?', const=True, help='é€‰æ‹©æ¨¡å‹å¹¶åŠ è½½å¯¹åº”é…ç½®ä¿¡æ¯')
parser.add_argument('--show-prompt', nargs='?', const=True, help='æ˜¾ç¤ºå®Œæ•´ä¸Šä¸‹æ–‡')
parser.add_argument('--warning-off', action='store_true', help="å…³é—­ Python è­¦å‘Š")
args = parser.parse_args()

if args.warning_off:
    import warnings
    warnings.filterwarnings("ignore", category=UserWarning)

if args.listen:
    host = "0.0.0.0"
else:
    host = "127.0.0.1"
    

function.hardware_detection()
function.torch_version()


self_dir = os.path.abspath(os.path.dirname(__file__))
history_dir = os.path.join(self_dir, 'histories')
now_json_file = os.path.join(history_dir, 'NewChat.json')

main_config = function.load_config(os.path.join(self_dir, 'main_config.yaml'))
model_config = function.load_config(os.path.join(self_dir, 'model_config.yaml'))
models_dir = os.path.join(self_dir, 'models')

main_name = main_config["main_name"]
main_description = main_config["main_description"]

model_name = list(model_config)[0]
if args.model_set:
    model_card = args.model_set
    if model_card in list(model_config):
        model_name = model_card
    else:
        print(f"--model-set å‚æ•°ä¸­çš„æ¨¡å‹åç§° {model_card} ä¸æ­£ç¡®ï¼Œå°†ä½¿ç”¨é»˜è®¤æ¨¡å‹é…ç½®")
        print(f"æ­£ç¡®å¡«å†™çš„åº”è¯¥æ˜¯ {list(model_config)[0]} æˆ– {list(model_config)[1]}")

print(f"Load model configï¼š{model_name}")

description = model_config[model_name]["description"]
dtypes = model_config[model_name]["dtypes"]
max_input_tokens = model_config[model_name]["max_input_tokens"]
max_generate_tokens = model_config[model_name]["max_generate_tokens"]
max_msg = model_config[model_name]["max_msg"]
max_file_content = model_config[model_name]["max_file_content"]
system_set = model_config[model_name]["system_set"]


up_file_type = model_config[model_name]["up_file_type"]

json_file_list, first_message_list = function.get_json_list(history_dir) # json_file_dict

file_content_dict = {}
samples = [[t] for t in first_message_list]

model = None
tokenizer = None
intercept = False
dividing_line = "-" * 50

# ç”¨ä¸ç€çš„
def dummy(v):
    print(f"ä½ è¾“å…¥äº†ï¼š{v}")
    return v

def role_tip(role):
    tip = f"ä½ åˆšåˆšé€‰æ‹©äº†è§’è‰² {role}ï¼Œå»ºè®®åˆ‡æ¢åˆ°æ–°å¯¹è¯ä½¿ç”¨ã€‚"
    print(tip)
    gr.Info(tip, duration=4)
    
# è¿™æ˜¯èŠå¤©åˆ—è¡¨é€‰æ‹©åè§¦å‘çš„ï¼Œè·å–æ–‡ä»¶å†…å®¹ç»™æœºå™¨äººå’Œç½‘é¡µç¼“å­˜
def get_json_content(index):
    global now_json_file
    now_json_file = json_file_list[index]
    json_content = function.load_json_content(now_json_file)
    # å°†è¯»å–åˆ°çš„è®°å½•æ–‡ä»¶å†…å®¹ï¼Œå‘é€ç»™æœºå™¨äººå’Œç½‘é¡µå­˜å‚¨
    return json_content, json_content

# æ¸…ç©ºåï¼ŒèŠå¤©è®°å½•ä¸ºç©ºï¼Œå°±ä¼šè‡ªåŠ¨åˆ›å»ºä¸€ä¸ªæ–°çš„æ–‡ä»¶ï¼Œæ‰€ä»¥èŠå¤©è®°å½•å¹¶ä¸æ˜¯çœŸçš„è¢«æ¸…ç©º
def clear_conversation():
    return {}, [], []  

# ç»™å³ä¾§è¾¹æ ç½‘é¡µå­˜å‚¨çš„ï¼Œä¸€å¼€å§‹è¦åŠ è½½åˆå§‹å€¼ï¼Œå¦‚æ˜¯é€‰é¡¹åˆ™æ˜¯ç¬¬ä¸€ä¸ªï¼Œå¦‚æ˜¯æ•°å­—åˆ™ç›´æ¥ç»™
column_dict = {
    "model": list(model_config)[0],
    "dtype": dtypes[0],
    "role": list(system_set)[0],
    "max_tokens": max_input_tokens,
    "max_generate": max_generate_tokens,
    "max_msg": max_msg,
    "max_file_token": max_file_content
}
# èšç„¦æ–‡æœ¬è¾“å…¥æ¡†æ—¶è§¦å‘ï¼Œè·å– 7 ä¸ªé€‰é¡¹ï¼Œå¹¶æŠŠæ‰€é€‰å€¼ç»™å³ä¾§è¾¹æ ç½‘é¡µå­˜å‚¨
def storage_column(model, dtype, role, max_tokens, max_generate, max_msg, max_file_token):
    column_dict = {
        "model": model,
        "dtype": dtype,
        "role": role,
        "max_tokens": max_tokens,
        "max_generate": max_generate,
        "max_msg": max_msg,
        "max_file_token": max_file_token
    }
    return column_dict
# åˆ·æ–°ç½‘é¡µç”¨çš„ï¼Œç”¨äºè·å–æœ€æ–°èŠå¤©åˆ—è¡¨
def init_load(storage_state, storage_column_r):
    # é‡æ–°è·å–æ–‡ä»¶åˆ—è¡¨
    json_file_list, first_message_list = function.get_json_list(history_dir) 
    new_samples = [[t] for t in first_message_list]

    dict = storage_column_r
    # åŸæ ·å‘é€ç»™è‡ªå·±ï¼Œä¹ŸæŠŠè¯»å–åˆ°çš„åˆ—è¡¨ä¿¡æ¯å‘é€ç»™ä¼šè¯åˆ—è¡¨
    return storage_state, gr.update(samples=new_samples), dict["model"], dict["dtype"], dict["role"], dict["max_tokens"], dict["max_generate"], dict["max_msg"], dict["max_file_token"]

def clear_gpu(model):
    import gc, torch_musa
    del model
    gc.collect()
    torch_musa.empty_cache()

def load_model(model_path, dtype):
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    global model, tokenizer

    model_name = os.path.basename(model_path)

    if model is None or tokenizer is None:
        info = f"â³ æ­£åœ¨åŠ è½½æ¨¡å‹ {model_name} ..."
        print(info)
        gr.Info(info)
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=getattr(torch, dtype),
            device_map="auto"
        )
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

def file_size_warn(file_path, max_file_token):
    warn_info = ""
    intercept = False
    if os.path.getsize(file_path) > 2 * 1024 * 1024:
        warn_info = "âŒ æ–‡ä»¶è¿‡å¤§ï¼Œè¯·æ§åˆ¶åœ¨ 2MB ä»¥å†…ã€‚"

        intercept = True
    else:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                file_content = f.read()
            approx_tokens = function.rough_token_estimate(file_content)
            print(f"ä¸Šä¼ æ–‡ä»¶ä¼°ç®— token æ•°ï¼š{approx_tokens}")
            if approx_tokens > max_file_token: # è®°å¾—æ”¹æˆ 24000
                warn_info = "âŒ æ–‡ä»¶å†…å®¹è¿‡å¤šï¼Œè¯·é‡æ–°é€‰æ‹©æ–‡ä»¶ã€‚"
                intercept = True
        except Exception as e:
            warn_info = "âŒ æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œè¯·é‡æ–°é€‰æ‹©æ–‡ä»¶ã€‚"
            intercept = True
    return intercept, warn_info
    
# ä¼ ç»™æœºå™¨äººä¼ ç»™å†å²è®°å½•
def respond_user(message, chat_history, max_message, max_file_token):
    print(dividing_line)
    global now_json_file, json_file_list, intercept

    if not chat_history:
        now_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
        now_json_file = os.path.join(history_dir, f'{now_time}.json') 

    files = None
    intercept = False
    if isinstance(message, dict):
        text_message = message.get("text", "")
        if files is None:
            files = message.get("files", [])
    else:
        text_message = message
        
    # å„ç§æ‹¦æˆª
    if not text_message.strip() and not files:
        intercept = True
        warn_info = "âŒ ç©ºä¿¡æ¯"
        
    approx_tokens = function.rough_token_estimate(text_message)
    if approx_tokens > max_message:
        intercept = True
        warn_info = "âŒ ä¸€æ¬¡æ€§å‘é€ä¿¡æ¯è¿‡å¤šï¼Œè¯·å‡å°‘æ–‡å­—æˆ–åˆ†æ®µå‘é€"
    
    if files:
        intercept, warn_info = file_size_warn(files[0], max_file_token)

    if intercept:
        print(warn_info)
        gr.Info(warn_info, duration=2)

        json_file_list, first_message_list = function.get_json_list(history_dir) # æ— å¥ˆå•Š
        samples=[[t] for t in first_message_list]
        return "", chat_history, chat_history, gr.update(samples=samples)

    chat_history.append({"role": "user", "content": text_message})
    
    if files:
        if isinstance(files, list):
            files = tuple(files) # å¼ºè¡Œè½¬ä¸ºå…ƒç»„ï¼Œå€’æ˜¯ç»™æ¨¡å‹å‰è¿›è¡Œç»Ÿä¸€æ•´ç†
        chat_history.append({"role": "user", "content": files})
    # print("å¾—åˆ° now_json_file åç§°ï¼š",now_json_file)

    with open(now_json_file, "w", encoding="utf-8") as f:
        json.dump(chat_history, f, ensure_ascii=False, indent=2)
    # è¿™æ˜¯æ­£ç¡®çš„ï¼Œæ¯æ¬¡èŠå¤©éƒ½è¦é‡æ–°è·å–ä¸€æ¬¡èŠå¤©åˆ—è¡¨ï¼Œæœ‰åŠ©äºæ›´æ–°åˆ—è¡¨æ’ä½
    json_file_list, first_message_list = function.get_json_list(history_dir) 
    samples=[[t] for t in first_message_list]

    return "", chat_history, chat_history, gr.update(samples=samples)

def respond_bot(chat_history, model_select, role_select, dtype_select, do_sample, max_tokens_input, max_tokens_generate, record_model_name):
    global model, tokenizer
    
    role_set = system_set.get(role_select)
    print(f"ğŸ’» ç³»ç»Ÿè§’è‰²è®¾å®šï¼š{role_select}")
    if not intercept: # å¦‚æœä¿¡æ¯è¢«æ‹¦æˆªæ‹¦æˆªäº†ï¼Œè¿™é‡Œæ‰æ‰§è¡Œ
        global now_json_file, json_file_list
        system_message = [{"role": "system", "content": role_set.strip()}]
        to_model_messages = system_message + function.model_mes_manage(chat_history)
        last_msg = to_model_messages[-1]
        print(f"ğŸ˜ {last_msg.get('role')} æé—®ï¼š\n{last_msg.get('content')}")
        if args.ui_test:
            bot_message = random.choice([
                f"ï¼ˆUIæµ‹è¯•æ¨¡å¼ä¸‹çš„æ¨¡æ‹Ÿå›å¤ï¼‰ğŸ¤– æˆ‘æ˜¯é€šä¹‰åƒé—®æ¨¡å‹ {model_name}",
                f"ï¼ˆUIæµ‹è¯•æ¨¡å¼ä¸‹çš„æ¨¡æ‹Ÿå›å¤ï¼‰ğŸ˜ {description}",
                "ï¼ˆUIæµ‹è¯•æ¨¡å¼ä¸‹çš„æ¨¡æ‹Ÿå›å¤ï¼‰ğŸ¤£ å…¶å®æ˜¯ä¸ªå‡æœºå™¨äººï¼Œå“ˆå“ˆ"
            ]) # ç®€å•æ¨¡æ‹Ÿæœºå™¨äººï¼Œéšæœºä¸€ä¸ªè¿›è¡Œå›å¤
            
            time.sleep(0.8)
            print(f"ğŸ¤– æ¨¡æ‹ŸAIå›å¤ï¼š\n{bot_message}")
            chat_history.append({"role": "assistant", "content": bot_message}) # æ¨¡æ‹Ÿçš„æœºå™¨äººå›å¤
        else:
            # åˆ†æè¯å…ƒå¹¶æˆªæ–­ï¼Œå¯ä»¥çš„ï¼Œæš‚æ—¶ç°æ‰
            from transformers import AutoTokenizer
            if do_sample == "å¯ç”¨":
                sample_Enable = True
            else:
                sample_Enable = False
            model_path = os.path.join(models_dir, model_select)
            repo_id = model_config[model_select]["ms_repoid"]
            absence = repo_download.check_repo_wholeness(repo_id, model_path)
            if absence:
                info_t = "æ£€æµ‹æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨æˆ–ä¸å®Œæ•´ï¼Œå¼€å§‹ä¸‹è½½æ¨¡å‹æ–‡ä»¶..."
                print(info_t)
                gr.Info(info_t, duration=6)
                wholeness_files = repo_download.download_ms(repo_id, model_path)
                if wholeness_files:
                    print("âœ… æ¨¡å‹å·²ä¸‹è½½åˆ°ï¼š", model_path)
            
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            reserved = function.smart_truncate_conversation(to_model_messages, tokenizer, max_tokens=max_tokens_input, min_reserve=2)
            # print("æˆªæ–­åä¿¡æ¯ï¼š\n", reserved)
            if model_select != record_model_name and model is not None:
                clear_gpu(model)
                model=None
                tokenizer=None
            
            load_model(model_path, dtype_select)
            text = tokenizer.apply_chat_template(reserved, tokenize=False, add_generation_prompt=True)
            if args.show_prompt:
                print("ğŸ“ ä¼ ç»™æ¨¡å‹çš„å®Œæ•´promptï¼š\n", text)
            
            inputs = tokenizer([text], return_tensors="pt").to(model.device)
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=max_tokens_generate,
                do_sample=sample_Enable,
            )
            generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)]
            response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            print(f"AIå›å¤ï¼š\n{response}")
            chat_history.append({"role": "assistant", "content": response})
    
        with open(now_json_file, "w", encoding="utf-8") as f:
            json.dump(chat_history, f, ensure_ascii=False, indent=2)
            
    json_file_list, first_message_list = function.get_json_list(history_dir) # è¿™æ˜¯æ­£ç¡®çš„ï¼Œæ¯æ¬¡èŠå¤©éƒ½è¦é‡æ–°è·å–ä¸€æ¬¡èŠå¤©åˆ—è¡¨ï¼Œæœ‰åŠ©äºæ›´æ–°åˆ—è¡¨æ’ä½
    samples=[[t] for t in first_message_list]
    

    return "", chat_history, chat_history, gr.update(samples=samples), model_select


with gr.Blocks(theme="soft") as demo:
    storage = gr.BrowserState([])
    storage_column_r = gr.BrowserState(column_dict)
    record_model_name = gr.State()
    # gr.HTML("<h1 style='text-align: center;'>ğŸ“Qwen ChatBot</h1>")
    gr.HTML(f"""
    <div style="display: flex; align-items: center; justify-content: center; gap: 12px; margin-bottom: 1.5em;">
      <img src="https://img.alicdn.com/imgextra/i4/O1CN01EfJVFQ1uZPd7W4W6i_!!6000000006051-2-tps-112-112.png" 
           alt="Logo" style="width: 48px; height: 48px;">
      <div>
        <div style="font-size: 24px; font-weight: bold;">{main_name}</div>
        <div style="font-size: 14px; color: gray;">{main_description}</div>
      </div>
    </div>
    """)
    # json_dict = gr.Textbox(visible=False) # ä¸çŸ¥é“æ€ä¹ˆä½¿ç”¨äº†
    with gr.Row():
        with gr.Column(scale=1, min_width=220):
            # æ¥è‡ª gradio/gradio/chat_interface.py 289è¡Œ
            new_chat_button = gr.Button(
                "æ–°å¯¹è¯",
                variant="primary",
                size="md",
                icon=utils.get_icon_path("plus.svg"),
            )
            # å·¦ä¾§è¾¹æ å¯¹è¯åˆ—è¡¨
            sess_list = gr.Dataset(
                components=[gr.Textbox(label='èŠå¤©è®°å½•', visible=False, lines=1, max_lines=1)],
                samples=samples,
                samples_per_page=14,
                type="index", layout="table", show_label=False
            )
            
        with gr.Column(scale=4):
            # å…è®¸ä¿®æ”¹ç”¨æˆ·å†…å®¹ editable="user"ï¼Œè¿™ä¸ªè¿˜ä¸çŸ¥é“æ€ä¹ˆç”¨ï¼Œæš‚æ—¶ä¸è¦
            chatbot = gr.Chatbot(type="messages", label='Qwen ChatBot', show_copy_button=False, height=560)
            msg = gr.MultimodalTextbox(placeholder="è¾“å…¥æ¶ˆæ¯æˆ–ä¸Šä¼ æ–‡ä»¶...", show_label=False, file_types=up_file_type)
        with gr.Column(scale=1, min_width=280):
            model_select = gr.Dropdown(choices=list(model_config), value=model_name, label="æ¨¡å‹",)
            role_select = gr.Dropdown(choices=list(system_set), label="è§’è‰²é€‰æ‹©",)
            dtype_select = gr.Dropdown(choices=dtypes, label="æ¨¡å‹ç²¾åº¦è®¾ç½®",)
            with gr.Accordion("æ›´å¤šè®¾ç½®", open=False):
                do_sample = gr.Dropdown(choices=["å¯ç”¨", "ç¦ç”¨"], label="éšæœºé‡‡æ ·",)
                max_tokens_input = gr.Number(value=max_input_tokens, label='æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦', info='é™åˆ¶é•¿åº¦ä»¥é¿å…çˆ†æ˜¾å­˜', precision=0, step=64)
                max_tokens_generate = gr.Number(value=max_generate_tokens, label='æœ€å¤§ç”Ÿæˆtoken', precision=0, step=64)
                max_message = gr.Number(value=max_msg, label='æœ€å¤§å•æ¬¡å‘é€ä¿¡æ¯é‡', precision=0, step=64)
                max_file_token = gr.Number(value=max_file_content, label='æœ€å¤§æ–‡ä»¶ä¿¡æ¯é‡', precision=0, step=64)
    # 1.ä¿¡æ¯ç»™ chatbot
    submit_event = msg.submit(
        respond_user,
        inputs=[msg, chatbot, max_message, max_file_token],
        outputs=[msg, chatbot, storage, sess_list]
    )

    # 2.å†è®©æ¨¡å‹å›å¤
    submit_event.then(
        respond_bot,
        inputs=[chatbot, model_select, role_select, dtype_select, do_sample, max_tokens_input, max_tokens_generate, record_model_name],
        outputs=[msg, chatbot, storage, sess_list, record_model_name]
    )
    # å¤šæ¨¡æ€æ–‡æœ¬æ¡†èšç„¦è§¦å‘
    msg.focus(
        fn=storage_column,
        inputs=[model_select, dtype_select, role_select, max_tokens_input, max_tokens_generate, max_message, max_file_token],
        outputs=[storage_column_r]
    )

    role_select.select(fn=role_tip, inputs=[role_select], outputs=None)
    sess_list.select(get_json_content, sess_list, [chatbot, storage])
    new_chat_button.click(fn=clear_conversation, inputs=[], outputs=[msg, chatbot, storage],)

    demo.load(
        init_load,
        inputs=[storage, storage_column_r],
        outputs=[chatbot, sess_list, model_select, dtype_select, role_select, max_tokens_input, max_tokens_generate, max_message, max_file_token]
    )


if __name__ == "__main__":
    demo.launch(server_name=host, server_port=int(args.port),)